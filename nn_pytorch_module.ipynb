{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:18.838397Z",
     "start_time": "2025-02-02T10:36:17.049085Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:18.857778Z",
     "start_time": "2025-02-02T10:36:18.841294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('breast_cancer_data.csv')\n",
    "data.head()"
   ],
   "id": "b458d0296a94b35d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:18.922424Z",
     "start_time": "2025-02-02T10:36:18.919710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = data.iloc[:, 1:]\n",
    "Y = data.diagnosis\n",
    "\n",
    "X.shape, Y.shape"
   ],
   "id": "de9474e93cc9c819",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:18.949642Z",
     "start_time": "2025-02-02T10:36:18.946327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "Y"
   ],
   "id": "5ef5578ea3ede88a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:18.985652Z",
     "start_time": "2025-02-02T10:36:18.981146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ],
   "id": "b0d272cc35431d96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n",
       "         2.75062224,  1.93701461],\n",
       "       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n",
       "        -0.24388967,  0.28118999],\n",
       "       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n",
       "         1.152255  ,  0.20139121],\n",
       "       ...,\n",
       "       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n",
       "        -1.10454895, -0.31840916],\n",
       "       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n",
       "         1.91908301,  2.21963528],\n",
       "       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n",
       "        -0.04813821, -0.75120669]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.005483Z",
     "start_time": "2025-02-02T10:36:19.002204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "id": "f2075e00bf251fda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455,), (114,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.029775Z",
     "start_time": "2025-02-02T10:36:19.027505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert from nupy to tensor\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "x_test_tensor = torch.from_numpy(x_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()"
   ],
   "id": "ee89842e5cc98f29",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.062286Z",
     "start_time": "2025-02-02T10:36:19.059957Z"
    }
   },
   "cell_type": "code",
   "source": "x_train_tensor.shape, x_test_tensor.shape, y_train_tensor.shape, y_test_tensor.shape",
   "id": "1518957cc3d8c930",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([455, 30]),\n",
       " torch.Size([114, 30]),\n",
       " torch.Size([455]),\n",
       " torch.Size([114]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Building the neural network using nn module\n",
    "    - NN: core library that provide a wide range of classes and designed to help devs to build neural networks\n",
    "        - Layers\n",
    "        - Activation Functions\n",
    "        - Loss Functions\n",
    "        - Container Modules: `nn.Sequential` container to stack layers in order\n",
    "        - Regularization and Dropout\n",
    "- Using built-in activation functions\n",
    "- Using built-in loss function\n",
    "- Using built-in optimizers"
   ],
   "id": "85ae8ac159f19379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.096773Z",
     "start_time": "2025-02-02T10:36:19.094636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ],
   "id": "c65ec4d4b69bd9d2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.121144Z",
     "start_time": "2025-02-02T10:36:19.118746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MyClassifier(num_features=x_train_tensor.shape[1])\n",
    "\n",
    "# model.forward(x_train_tensor)\n",
    "# In nn module __call__ is overwritten therefore we don't need to call mode.forward()\n",
    "\n",
    "# standard way\n",
    "y_pred = model(x_train_tensor)"
   ],
   "id": "9ec2bc0a9608085b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.128250Z",
     "start_time": "2025-02-02T10:36:19.123751Z"
    }
   },
   "cell_type": "code",
   "source": "print(y_pred)",
   "id": "e0ab1de0c6300152",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1634],\n",
      "        [0.7257],\n",
      "        [0.2603],\n",
      "        [0.2129],\n",
      "        [0.1619],\n",
      "        [0.2197],\n",
      "        [0.5243],\n",
      "        [0.3549],\n",
      "        [0.2957],\n",
      "        [0.7696],\n",
      "        [0.3080],\n",
      "        [0.8943],\n",
      "        [0.4194],\n",
      "        [0.2006],\n",
      "        [0.3790],\n",
      "        [0.6285],\n",
      "        [0.8304],\n",
      "        [0.3066],\n",
      "        [0.4650],\n",
      "        [0.2648],\n",
      "        [0.5858],\n",
      "        [0.3768],\n",
      "        [0.4338],\n",
      "        [0.2950],\n",
      "        [0.4354],\n",
      "        [0.2999],\n",
      "        [0.3121],\n",
      "        [0.8681],\n",
      "        [0.2743],\n",
      "        [0.4747],\n",
      "        [0.3613],\n",
      "        [0.3847],\n",
      "        [0.6520],\n",
      "        [0.8487],\n",
      "        [0.3961],\n",
      "        [0.5379],\n",
      "        [0.4909],\n",
      "        [0.4658],\n",
      "        [0.1978],\n",
      "        [0.4542],\n",
      "        [0.2573],\n",
      "        [0.9338],\n",
      "        [0.7423],\n",
      "        [0.3019],\n",
      "        [0.4962],\n",
      "        [0.3733],\n",
      "        [0.7171],\n",
      "        [0.2342],\n",
      "        [0.2508],\n",
      "        [0.5155],\n",
      "        [0.4497],\n",
      "        [0.2633],\n",
      "        [0.4029],\n",
      "        [0.4550],\n",
      "        [0.2455],\n",
      "        [0.2543],\n",
      "        [0.3565],\n",
      "        [0.5113],\n",
      "        [0.5212],\n",
      "        [0.5805],\n",
      "        [0.3614],\n",
      "        [0.4374],\n",
      "        [0.6329],\n",
      "        [0.4361],\n",
      "        [0.3375],\n",
      "        [0.4876],\n",
      "        [0.4075],\n",
      "        [0.3218],\n",
      "        [0.1848],\n",
      "        [0.6660],\n",
      "        [0.7733],\n",
      "        [0.3544],\n",
      "        [0.3288],\n",
      "        [0.3748],\n",
      "        [0.5816],\n",
      "        [0.4729],\n",
      "        [0.7648],\n",
      "        [0.3792],\n",
      "        [0.3939],\n",
      "        [0.3628],\n",
      "        [0.2980],\n",
      "        [0.6605],\n",
      "        [0.4129],\n",
      "        [0.2295],\n",
      "        [0.5855],\n",
      "        [0.3326],\n",
      "        [0.5370],\n",
      "        [0.1299],\n",
      "        [0.6673],\n",
      "        [0.3200],\n",
      "        [0.6854],\n",
      "        [0.7957],\n",
      "        [0.4453],\n",
      "        [0.3292],\n",
      "        [0.2974],\n",
      "        [0.3541],\n",
      "        [0.2202],\n",
      "        [0.2872],\n",
      "        [0.1650],\n",
      "        [0.3845],\n",
      "        [0.1901],\n",
      "        [0.3072],\n",
      "        [0.3752],\n",
      "        [0.3564],\n",
      "        [0.2984],\n",
      "        [0.5682],\n",
      "        [0.3941],\n",
      "        [0.5840],\n",
      "        [0.7103],\n",
      "        [0.5241],\n",
      "        [0.2923],\n",
      "        [0.2191],\n",
      "        [0.9460],\n",
      "        [0.4110],\n",
      "        [0.4269],\n",
      "        [0.3278],\n",
      "        [0.3001],\n",
      "        [0.5479],\n",
      "        [0.5354],\n",
      "        [0.4343],\n",
      "        [0.4533],\n",
      "        [0.5754],\n",
      "        [0.9781],\n",
      "        [0.3726],\n",
      "        [0.6293],\n",
      "        [0.8581],\n",
      "        [0.4576],\n",
      "        [0.5152],\n",
      "        [0.7819],\n",
      "        [0.5175],\n",
      "        [0.2570],\n",
      "        [0.3652],\n",
      "        [0.6766],\n",
      "        [0.4781],\n",
      "        [0.5518],\n",
      "        [0.2709],\n",
      "        [0.4402],\n",
      "        [0.5453],\n",
      "        [0.3682],\n",
      "        [0.2933],\n",
      "        [0.3658],\n",
      "        [0.0881],\n",
      "        [0.2983],\n",
      "        [0.7882],\n",
      "        [0.4728],\n",
      "        [0.2781],\n",
      "        [0.4389],\n",
      "        [0.2749],\n",
      "        [0.5690],\n",
      "        [0.7005],\n",
      "        [0.8201],\n",
      "        [0.4437],\n",
      "        [0.2425],\n",
      "        [0.4089],\n",
      "        [0.4076],\n",
      "        [0.5237],\n",
      "        [0.3289],\n",
      "        [0.3654],\n",
      "        [0.3629],\n",
      "        [0.8747],\n",
      "        [0.4125],\n",
      "        [0.2878],\n",
      "        [0.2570],\n",
      "        [0.1969],\n",
      "        [0.2853],\n",
      "        [0.8517],\n",
      "        [0.4891],\n",
      "        [0.3172],\n",
      "        [0.2281],\n",
      "        [0.2941],\n",
      "        [0.3936],\n",
      "        [0.4172],\n",
      "        [0.2511],\n",
      "        [0.4883],\n",
      "        [0.3928],\n",
      "        [0.3314],\n",
      "        [0.3338],\n",
      "        [0.3668],\n",
      "        [0.4157],\n",
      "        [0.3976],\n",
      "        [0.6378],\n",
      "        [0.4657],\n",
      "        [0.2900],\n",
      "        [0.3335],\n",
      "        [0.3742],\n",
      "        [0.5909],\n",
      "        [0.8378],\n",
      "        [0.1790],\n",
      "        [0.4181],\n",
      "        [0.5569],\n",
      "        [0.4884],\n",
      "        [0.6583],\n",
      "        [0.6572],\n",
      "        [0.5724],\n",
      "        [0.2724],\n",
      "        [0.3306],\n",
      "        [0.3235],\n",
      "        [0.7651],\n",
      "        [0.2582],\n",
      "        [0.2762],\n",
      "        [0.6678],\n",
      "        [0.3004],\n",
      "        [0.1596],\n",
      "        [0.5053],\n",
      "        [0.5064],\n",
      "        [0.4048],\n",
      "        [0.9702],\n",
      "        [0.4978],\n",
      "        [0.9452],\n",
      "        [0.3978],\n",
      "        [0.3468],\n",
      "        [0.5884],\n",
      "        [0.7966],\n",
      "        [0.3539],\n",
      "        [0.3827],\n",
      "        [0.8320],\n",
      "        [0.2910],\n",
      "        [0.7452],\n",
      "        [0.9633],\n",
      "        [0.5009],\n",
      "        [0.8718],\n",
      "        [0.6033],\n",
      "        [0.4405],\n",
      "        [0.4272],\n",
      "        [0.8207],\n",
      "        [0.7670],\n",
      "        [0.9545],\n",
      "        [0.3149],\n",
      "        [0.2474],\n",
      "        [0.4605],\n",
      "        [0.4870],\n",
      "        [0.3755],\n",
      "        [0.2598],\n",
      "        [0.7892],\n",
      "        [0.8390],\n",
      "        [0.7429],\n",
      "        [0.6277],\n",
      "        [0.3970],\n",
      "        [0.3476],\n",
      "        [0.3306],\n",
      "        [0.3337],\n",
      "        [0.5154],\n",
      "        [0.4249],\n",
      "        [0.4118],\n",
      "        [0.3227],\n",
      "        [0.7563],\n",
      "        [0.8506],\n",
      "        [0.6603],\n",
      "        [0.4743],\n",
      "        [0.6714],\n",
      "        [0.2313],\n",
      "        [0.4416],\n",
      "        [0.1699],\n",
      "        [0.0980],\n",
      "        [0.2446],\n",
      "        [0.8305],\n",
      "        [0.3601],\n",
      "        [0.2680],\n",
      "        [0.8368],\n",
      "        [0.6548],\n",
      "        [0.5741],\n",
      "        [0.7503],\n",
      "        [0.4605],\n",
      "        [0.8324],\n",
      "        [0.2612],\n",
      "        [0.3126],\n",
      "        [0.2859],\n",
      "        [0.4891],\n",
      "        [0.2456],\n",
      "        [0.4451],\n",
      "        [0.6757],\n",
      "        [0.3118],\n",
      "        [0.4795],\n",
      "        [0.7593],\n",
      "        [0.5098],\n",
      "        [0.4185],\n",
      "        [0.3335],\n",
      "        [0.1702],\n",
      "        [0.3628],\n",
      "        [0.3587],\n",
      "        [0.6518],\n",
      "        [0.5480],\n",
      "        [0.3665],\n",
      "        [0.3124],\n",
      "        [0.5119],\n",
      "        [0.4388],\n",
      "        [0.5943],\n",
      "        [0.6736],\n",
      "        [0.2344],\n",
      "        [0.6489],\n",
      "        [0.5001],\n",
      "        [0.3587],\n",
      "        [0.8294],\n",
      "        [0.4309],\n",
      "        [0.4737],\n",
      "        [0.3152],\n",
      "        [0.3714],\n",
      "        [0.3215],\n",
      "        [0.3259],\n",
      "        [0.5715],\n",
      "        [0.3764],\n",
      "        [0.2787],\n",
      "        [0.7341],\n",
      "        [0.4829],\n",
      "        [0.4459],\n",
      "        [0.5620],\n",
      "        [0.4686],\n",
      "        [0.2613],\n",
      "        [0.4895],\n",
      "        [0.4841],\n",
      "        [0.2656],\n",
      "        [0.1755],\n",
      "        [0.4888],\n",
      "        [0.2201],\n",
      "        [0.6169],\n",
      "        [0.7065],\n",
      "        [0.3248],\n",
      "        [0.5535],\n",
      "        [0.7338],\n",
      "        [0.2689],\n",
      "        [0.8331],\n",
      "        [0.8287],\n",
      "        [0.3303],\n",
      "        [0.4876],\n",
      "        [0.5211],\n",
      "        [0.2711],\n",
      "        [0.4773],\n",
      "        [0.5025],\n",
      "        [0.3888],\n",
      "        [0.5048],\n",
      "        [0.3527],\n",
      "        [0.2564],\n",
      "        [0.3072],\n",
      "        [0.2167],\n",
      "        [0.5526],\n",
      "        [0.2338],\n",
      "        [0.6254],\n",
      "        [0.7555],\n",
      "        [0.5234],\n",
      "        [0.4599],\n",
      "        [0.3261],\n",
      "        [0.7223],\n",
      "        [0.8023],\n",
      "        [0.3034],\n",
      "        [0.5672],\n",
      "        [0.7174],\n",
      "        [0.2677],\n",
      "        [0.2277],\n",
      "        [0.3290],\n",
      "        [0.8404],\n",
      "        [0.2403],\n",
      "        [0.7097],\n",
      "        [0.6642],\n",
      "        [0.5430],\n",
      "        [0.3528],\n",
      "        [0.5048],\n",
      "        [0.3592],\n",
      "        [0.2681],\n",
      "        [0.5384],\n",
      "        [0.2746],\n",
      "        [0.3186],\n",
      "        [0.8712],\n",
      "        [0.2659],\n",
      "        [0.7603],\n",
      "        [0.3895],\n",
      "        [0.1414],\n",
      "        [0.3832],\n",
      "        [0.4366],\n",
      "        [0.2616],\n",
      "        [0.3313],\n",
      "        [0.3815],\n",
      "        [0.2945],\n",
      "        [0.3271],\n",
      "        [0.6274],\n",
      "        [0.9319],\n",
      "        [0.5768],\n",
      "        [0.9986],\n",
      "        [0.4384],\n",
      "        [0.2146],\n",
      "        [0.3560],\n",
      "        [0.4418],\n",
      "        [0.9981],\n",
      "        [0.4898],\n",
      "        [0.6151],\n",
      "        [0.3300],\n",
      "        [0.3635],\n",
      "        [0.3023],\n",
      "        [0.4701],\n",
      "        [0.3054],\n",
      "        [0.6625],\n",
      "        [0.8208],\n",
      "        [0.3505],\n",
      "        [0.3433],\n",
      "        [0.4065],\n",
      "        [0.1603],\n",
      "        [0.5252],\n",
      "        [0.5358],\n",
      "        [0.3147],\n",
      "        [0.3533],\n",
      "        [0.7495],\n",
      "        [0.7585],\n",
      "        [0.3474],\n",
      "        [0.3234],\n",
      "        [0.4138],\n",
      "        [0.3792],\n",
      "        [0.4903],\n",
      "        [0.2651],\n",
      "        [0.3266],\n",
      "        [0.8322],\n",
      "        [0.5277],\n",
      "        [0.9482],\n",
      "        [0.3318],\n",
      "        [0.3779],\n",
      "        [0.4896],\n",
      "        [0.5009],\n",
      "        [0.4836],\n",
      "        [0.1822],\n",
      "        [0.3820],\n",
      "        [0.8804],\n",
      "        [0.5313],\n",
      "        [0.3807],\n",
      "        [0.1469],\n",
      "        [0.4716],\n",
      "        [0.4291],\n",
      "        [0.3488],\n",
      "        [0.3265],\n",
      "        [0.4132],\n",
      "        [0.2396],\n",
      "        [0.7478],\n",
      "        [0.2722],\n",
      "        [0.2822],\n",
      "        [0.2978],\n",
      "        [0.3361],\n",
      "        [0.3009],\n",
      "        [0.5359],\n",
      "        [0.5137],\n",
      "        [0.3987],\n",
      "        [0.8255],\n",
      "        [0.7542],\n",
      "        [0.5417],\n",
      "        [0.2470],\n",
      "        [0.4502],\n",
      "        [0.8209],\n",
      "        [0.6133],\n",
      "        [0.4052],\n",
      "        [0.5390],\n",
      "        [0.2379],\n",
      "        [0.4458],\n",
      "        [0.7384],\n",
      "        [0.3508],\n",
      "        [0.1702],\n",
      "        [0.2942],\n",
      "        [0.4944],\n",
      "        [0.3973],\n",
      "        [0.3695]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.143705Z",
     "start_time": "2025-02-02T10:36:19.141449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# weights\n",
    "print(model.linear.weight)"
   ],
   "id": "a98ac838581f3fc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0102, -0.0325,  0.1008,  0.1693, -0.0714,  0.1468,  0.1789,  0.0957,\n",
      "         -0.1768,  0.0938,  0.1159,  0.1575,  0.0618,  0.1307, -0.0839,  0.0252,\n",
      "          0.0029, -0.1108, -0.0085, -0.1221,  0.1148, -0.0905,  0.1093,  0.0911,\n",
      "         -0.0538, -0.0964,  0.0126,  0.0705, -0.1778, -0.1521]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:19.159495Z",
     "start_time": "2025-02-02T10:36:19.157263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# linear\n",
    "print(model.linear.bias)"
   ],
   "id": "48b1bea3ee5be022",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1061], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Print model summary:\n",
    "- Just like in Keras we had model.summary() in PyTorch we have a library `torchinfo` to output  the model summary."
   ],
   "id": "9faabf215fff6607"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.083211Z",
     "start_time": "2025-02-02T10:36:19.172418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install torchinfo --quiet\n",
    "from torchinfo import summary"
   ],
   "id": "79876313830570f2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.103488Z",
     "start_time": "2025-02-02T10:36:20.099593Z"
    }
   },
   "cell_type": "code",
   "source": "summary(model, input_size = x_train_tensor.shape)",
   "id": "81f6f82ef38c6767",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyClassifier                             [455, 1]                  --\n",
       "├─Linear: 1-1                            [455, 1]                  31\n",
       "├─Sigmoid: 1-2                           [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 31\n",
       "Trainable params: 31\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Trainable Parameters: 30 features/columns + bias\n",
   "id": "28776fd0a44d7aec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.119736Z",
     "start_time": "2025-02-02T10:36:20.117365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelWithHiddenLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_features, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear1(features)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ],
   "id": "79df30eccdf7d8cd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.137756Z",
     "start_time": "2025-02-02T10:36:20.132832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = ModelWithHiddenLayer(x_train_tensor.shape[1])\n",
    "y_pred_2 = model2(x_train_tensor)\n",
    "\n",
    "print(y_pred_2)"
   ],
   "id": "22b430812b57b7cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3215],\n",
      "        [0.2509],\n",
      "        [0.3916],\n",
      "        [0.3316],\n",
      "        [0.2804],\n",
      "        [0.3298],\n",
      "        [0.2310],\n",
      "        [0.3581],\n",
      "        [0.3678],\n",
      "        [0.3497],\n",
      "        [0.3955],\n",
      "        [0.2887],\n",
      "        [0.3267],\n",
      "        [0.3826],\n",
      "        [0.3520],\n",
      "        [0.3649],\n",
      "        [0.2463],\n",
      "        [0.2997],\n",
      "        [0.3643],\n",
      "        [0.3319],\n",
      "        [0.3965],\n",
      "        [0.2985],\n",
      "        [0.3695],\n",
      "        [0.3508],\n",
      "        [0.3561],\n",
      "        [0.3903],\n",
      "        [0.3548],\n",
      "        [0.2804],\n",
      "        [0.3759],\n",
      "        [0.3723],\n",
      "        [0.3674],\n",
      "        [0.3754],\n",
      "        [0.3319],\n",
      "        [0.2643],\n",
      "        [0.3844],\n",
      "        [0.3616],\n",
      "        [0.3519],\n",
      "        [0.3431],\n",
      "        [0.3755],\n",
      "        [0.3461],\n",
      "        [0.3088],\n",
      "        [0.1241],\n",
      "        [0.2854],\n",
      "        [0.3651],\n",
      "        [0.3542],\n",
      "        [0.3486],\n",
      "        [0.3369],\n",
      "        [0.4156],\n",
      "        [0.3951],\n",
      "        [0.4156],\n",
      "        [0.2680],\n",
      "        [0.4138],\n",
      "        [0.3356],\n",
      "        [0.3256],\n",
      "        [0.3079],\n",
      "        [0.3132],\n",
      "        [0.4125],\n",
      "        [0.3547],\n",
      "        [0.4156],\n",
      "        [0.3887],\n",
      "        [0.3644],\n",
      "        [0.3314],\n",
      "        [0.3010],\n",
      "        [0.4156],\n",
      "        [0.3731],\n",
      "        [0.3744],\n",
      "        [0.4008],\n",
      "        [0.3593],\n",
      "        [0.4156],\n",
      "        [0.3891],\n",
      "        [0.3398],\n",
      "        [0.3726],\n",
      "        [0.4072],\n",
      "        [0.3873],\n",
      "        [0.3372],\n",
      "        [0.4116],\n",
      "        [0.2801],\n",
      "        [0.3727],\n",
      "        [0.3969],\n",
      "        [0.3428],\n",
      "        [0.3229],\n",
      "        [0.2800],\n",
      "        [0.3450],\n",
      "        [0.3873],\n",
      "        [0.2374],\n",
      "        [0.4006],\n",
      "        [0.3343],\n",
      "        [0.4156],\n",
      "        [0.1786],\n",
      "        [0.3515],\n",
      "        [0.2897],\n",
      "        [0.2943],\n",
      "        [0.3919],\n",
      "        [0.3540],\n",
      "        [0.4156],\n",
      "        [0.3657],\n",
      "        [0.4156],\n",
      "        [0.3649],\n",
      "        [0.4012],\n",
      "        [0.3426],\n",
      "        [0.3469],\n",
      "        [0.4156],\n",
      "        [0.3404],\n",
      "        [0.3722],\n",
      "        [0.3843],\n",
      "        [0.3346],\n",
      "        [0.3567],\n",
      "        [0.3408],\n",
      "        [0.2989],\n",
      "        [0.3150],\n",
      "        [0.3422],\n",
      "        [0.3803],\n",
      "        [0.2802],\n",
      "        [0.3621],\n",
      "        [0.3390],\n",
      "        [0.3498],\n",
      "        [0.3772],\n",
      "        [0.3965],\n",
      "        [0.3114],\n",
      "        [0.3732],\n",
      "        [0.3560],\n",
      "        [0.3565],\n",
      "        [0.1697],\n",
      "        [0.4156],\n",
      "        [0.3799],\n",
      "        [0.2843],\n",
      "        [0.3781],\n",
      "        [0.3512],\n",
      "        [0.3175],\n",
      "        [0.3320],\n",
      "        [0.3460],\n",
      "        [0.4156],\n",
      "        [0.2672],\n",
      "        [0.3933],\n",
      "        [0.3491],\n",
      "        [0.3773],\n",
      "        [0.4047],\n",
      "        [0.3720],\n",
      "        [0.3939],\n",
      "        [0.3693],\n",
      "        [0.4156],\n",
      "        [0.2084],\n",
      "        [0.3578],\n",
      "        [0.2738],\n",
      "        [0.3634],\n",
      "        [0.3358],\n",
      "        [0.3319],\n",
      "        [0.3610],\n",
      "        [0.3091],\n",
      "        [0.3610],\n",
      "        [0.3248],\n",
      "        [0.3091],\n",
      "        [0.3737],\n",
      "        [0.4156],\n",
      "        [0.4156],\n",
      "        [0.3669],\n",
      "        [0.4156],\n",
      "        [0.3619],\n",
      "        [0.4156],\n",
      "        [0.3281],\n",
      "        [0.3760],\n",
      "        [0.3733],\n",
      "        [0.3667],\n",
      "        [0.2016],\n",
      "        [0.4156],\n",
      "        [0.2349],\n",
      "        [0.3447],\n",
      "        [0.3242],\n",
      "        [0.3921],\n",
      "        [0.3991],\n",
      "        [0.3699],\n",
      "        [0.4120],\n",
      "        [0.3458],\n",
      "        [0.2251],\n",
      "        [0.4156],\n",
      "        [0.3746],\n",
      "        [0.3580],\n",
      "        [0.3658],\n",
      "        [0.3318],\n",
      "        [0.2858],\n",
      "        [0.3367],\n",
      "        [0.3371],\n",
      "        [0.3669],\n",
      "        [0.3849],\n",
      "        [0.3930],\n",
      "        [0.3518],\n",
      "        [0.2817],\n",
      "        [0.4106],\n",
      "        [0.3968],\n",
      "        [0.3214],\n",
      "        [0.3206],\n",
      "        [0.3063],\n",
      "        [0.4083],\n",
      "        [0.3676],\n",
      "        [0.2660],\n",
      "        [0.3825],\n",
      "        [0.3719],\n",
      "        [0.3527],\n",
      "        [0.3589],\n",
      "        [0.3789],\n",
      "        [0.3764],\n",
      "        [0.3749],\n",
      "        [0.2733],\n",
      "        [0.3569],\n",
      "        [0.3819],\n",
      "        [0.3153],\n",
      "        [0.1767],\n",
      "        [0.3860],\n",
      "        [0.2344],\n",
      "        [0.3332],\n",
      "        [0.3954],\n",
      "        [0.2849],\n",
      "        [0.2692],\n",
      "        [0.3944],\n",
      "        [0.3781],\n",
      "        [0.2761],\n",
      "        [0.3573],\n",
      "        [0.2431],\n",
      "        [0.0845],\n",
      "        [0.4156],\n",
      "        [0.1680],\n",
      "        [0.2104],\n",
      "        [0.3752],\n",
      "        [0.3882],\n",
      "        [0.3118],\n",
      "        [0.2791],\n",
      "        [0.2643],\n",
      "        [0.3510],\n",
      "        [0.3005],\n",
      "        [0.3412],\n",
      "        [0.3560],\n",
      "        [0.3819],\n",
      "        [0.4109],\n",
      "        [0.2854],\n",
      "        [0.2932],\n",
      "        [0.2995],\n",
      "        [0.3899],\n",
      "        [0.3986],\n",
      "        [0.3578],\n",
      "        [0.4113],\n",
      "        [0.3513],\n",
      "        [0.3395],\n",
      "        [0.3369],\n",
      "        [0.3540],\n",
      "        [0.3493],\n",
      "        [0.2705],\n",
      "        [0.3150],\n",
      "        [0.4004],\n",
      "        [0.3766],\n",
      "        [0.3157],\n",
      "        [0.3677],\n",
      "        [0.4006],\n",
      "        [0.3269],\n",
      "        [0.3955],\n",
      "        [0.3575],\n",
      "        [0.3043],\n",
      "        [0.3363],\n",
      "        [0.4115],\n",
      "        [0.2859],\n",
      "        [0.3559],\n",
      "        [0.3795],\n",
      "        [0.3794],\n",
      "        [0.3794],\n",
      "        [0.2612],\n",
      "        [0.2772],\n",
      "        [0.3707],\n",
      "        [0.3989],\n",
      "        [0.3438],\n",
      "        [0.4035],\n",
      "        [0.3714],\n",
      "        [0.3065],\n",
      "        [0.3991],\n",
      "        [0.3936],\n",
      "        [0.3414],\n",
      "        [0.4156],\n",
      "        [0.3211],\n",
      "        [0.4156],\n",
      "        [0.3742],\n",
      "        [0.3582],\n",
      "        [0.3567],\n",
      "        [0.2875],\n",
      "        [0.3784],\n",
      "        [0.3550],\n",
      "        [0.3973],\n",
      "        [0.3336],\n",
      "        [0.3222],\n",
      "        [0.3942],\n",
      "        [0.3013],\n",
      "        [0.3717],\n",
      "        [0.3669],\n",
      "        [0.4013],\n",
      "        [0.3775],\n",
      "        [0.1756],\n",
      "        [0.3309],\n",
      "        [0.3319],\n",
      "        [0.3636],\n",
      "        [0.3609],\n",
      "        [0.3809],\n",
      "        [0.3262],\n",
      "        [0.3989],\n",
      "        [0.4059],\n",
      "        [0.3494],\n",
      "        [0.2872],\n",
      "        [0.3705],\n",
      "        [0.3535],\n",
      "        [0.4156],\n",
      "        [0.1960],\n",
      "        [0.3895],\n",
      "        [0.3022],\n",
      "        [0.3128],\n",
      "        [0.3548],\n",
      "        [0.4156],\n",
      "        [0.2670],\n",
      "        [0.3546],\n",
      "        [0.3006],\n",
      "        [0.2995],\n",
      "        [0.4156],\n",
      "        [0.4033],\n",
      "        [0.3308],\n",
      "        [0.3881],\n",
      "        [0.3041],\n",
      "        [0.3091],\n",
      "        [0.4144],\n",
      "        [0.3548],\n",
      "        [0.3708],\n",
      "        [0.3543],\n",
      "        [0.3955],\n",
      "        [0.2224],\n",
      "        [0.3963],\n",
      "        [0.3844],\n",
      "        [0.4067],\n",
      "        [0.3981],\n",
      "        [0.3639],\n",
      "        [0.3485],\n",
      "        [0.3678],\n",
      "        [0.3503],\n",
      "        [0.2506],\n",
      "        [0.2926],\n",
      "        [0.3665],\n",
      "        [0.3427],\n",
      "        [0.3655],\n",
      "        [0.3416],\n",
      "        [0.3115],\n",
      "        [0.3994],\n",
      "        [0.3186],\n",
      "        [0.2646],\n",
      "        [0.4022],\n",
      "        [0.3752],\n",
      "        [0.3910],\n",
      "        [0.2632],\n",
      "        [0.4156],\n",
      "        [0.3370],\n",
      "        [0.2463],\n",
      "        [0.3378],\n",
      "        [0.3606],\n",
      "        [0.3601],\n",
      "        [0.3031],\n",
      "        [0.3165],\n",
      "        [0.3604],\n",
      "        [0.3690],\n",
      "        [0.3615],\n",
      "        [0.2633],\n",
      "        [0.3320],\n",
      "        [0.3004],\n",
      "        [0.3421],\n",
      "        [0.3020],\n",
      "        [0.3629],\n",
      "        [0.3874],\n",
      "        [0.3827],\n",
      "        [0.3301],\n",
      "        [0.3459],\n",
      "        [0.3498],\n",
      "        [0.3284],\n",
      "        [0.2947],\n",
      "        [0.2335],\n",
      "        [0.3308],\n",
      "        [0.1190],\n",
      "        [0.3393],\n",
      "        [0.3540],\n",
      "        [0.3410],\n",
      "        [0.3348],\n",
      "        [0.1286],\n",
      "        [0.3135],\n",
      "        [0.3613],\n",
      "        [0.3625],\n",
      "        [0.3526],\n",
      "        [0.4031],\n",
      "        [0.3449],\n",
      "        [0.3566],\n",
      "        [0.3862],\n",
      "        [0.3260],\n",
      "        [0.3974],\n",
      "        [0.3998],\n",
      "        [0.3813],\n",
      "        [0.2069],\n",
      "        [0.3944],\n",
      "        [0.3480],\n",
      "        [0.3185],\n",
      "        [0.3270],\n",
      "        [0.2329],\n",
      "        [0.3294],\n",
      "        [0.3841],\n",
      "        [0.3937],\n",
      "        [0.4156],\n",
      "        [0.3692],\n",
      "        [0.3447],\n",
      "        [0.3536],\n",
      "        [0.3523],\n",
      "        [0.2215],\n",
      "        [0.3536],\n",
      "        [0.2216],\n",
      "        [0.3953],\n",
      "        [0.3354],\n",
      "        [0.3327],\n",
      "        [0.3278],\n",
      "        [0.3531],\n",
      "        [0.3900],\n",
      "        [0.3967],\n",
      "        [0.2232],\n",
      "        [0.4156],\n",
      "        [0.3483],\n",
      "        [0.3092],\n",
      "        [0.3344],\n",
      "        [0.3631],\n",
      "        [0.3932],\n",
      "        [0.4156],\n",
      "        [0.3267],\n",
      "        [0.4156],\n",
      "        [0.2731],\n",
      "        [0.3940],\n",
      "        [0.3720],\n",
      "        [0.3219],\n",
      "        [0.3626],\n",
      "        [0.3308],\n",
      "        [0.3485],\n",
      "        [0.3501],\n",
      "        [0.3785],\n",
      "        [0.3756],\n",
      "        [0.2172],\n",
      "        [0.3419],\n",
      "        [0.3798],\n",
      "        [0.3888],\n",
      "        [0.3067],\n",
      "        [0.3401],\n",
      "        [0.3057],\n",
      "        [0.3933],\n",
      "        [0.2866],\n",
      "        [0.4156],\n",
      "        [0.2895],\n",
      "        [0.3841],\n",
      "        [0.3625],\n",
      "        [0.3759],\n",
      "        [0.3238],\n",
      "        [0.4071],\n",
      "        [0.3304]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.156371Z",
     "start_time": "2025-02-02T10:36:20.153230Z"
    }
   },
   "cell_type": "code",
   "source": "model2.linear1.weight, model2.linear2.weight",
   "id": "f0412c9e15653652",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.1320,  0.0504,  0.1808,  0.1188,  0.1814, -0.1766,  0.1618, -0.1053,\n",
       "          -0.0437,  0.1072, -0.1629, -0.0269,  0.1214,  0.1280,  0.1640,  0.0295,\n",
       "          -0.0859, -0.0154,  0.1741,  0.1770,  0.1799,  0.1225, -0.0919,  0.1252,\n",
       "           0.0946, -0.1450,  0.0978,  0.1695, -0.0740, -0.1784],\n",
       "         [ 0.1477, -0.1301, -0.0893,  0.0362,  0.1825, -0.0351, -0.1222,  0.1563,\n",
       "           0.1712, -0.1217,  0.0979,  0.0784,  0.0537, -0.1643, -0.1643,  0.0889,\n",
       "          -0.0365,  0.0436,  0.1266,  0.1572, -0.0355,  0.1459, -0.1453,  0.0098,\n",
       "           0.0766,  0.0919,  0.1268,  0.1000,  0.0420, -0.0821],\n",
       "         [ 0.0867,  0.1465,  0.0423,  0.0712,  0.0971, -0.1395, -0.1593, -0.0485,\n",
       "           0.1069, -0.1450, -0.0936, -0.0412, -0.1218,  0.0304, -0.1735, -0.0282,\n",
       "          -0.1111,  0.1157,  0.1305, -0.0995,  0.1179,  0.0463, -0.0843, -0.1808,\n",
       "           0.0844, -0.0633, -0.1330, -0.1126, -0.1060, -0.0188]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3341, -0.3634, -0.4149]], requires_grad=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- layer 1 : 30x3 weights\n",
    "- layer 2 : 3 weights"
   ],
   "id": "9453ad8e7b88af34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.174503Z",
     "start_time": "2025-02-02T10:36:20.171487Z"
    }
   },
   "cell_type": "code",
   "source": "summary(model2, input_size = x_train_tensor.shape)",
   "id": "ad143f1e71f4f301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ModelWithHiddenLayer                     [455, 1]                  --\n",
       "├─Linear: 1-1                            [455, 3]                  93\n",
       "├─ReLU: 1-2                              [455, 3]                  --\n",
       "├─Linear: 1-3                            [455, 1]                  4\n",
       "├─Sigmoid: 1-4                           [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 97\n",
       "Trainable params: 97\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialization and passing the layer is cumbersome job. We can use PyTorch `nn.Sequential` module.",
   "id": "d0e77794dc1dd7ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.192340Z",
     "start_time": "2025-02-02T10:36:20.190291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelWithSequentialModule(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "        return out\n"
   ],
   "id": "f3500ace821e416e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.213058Z",
     "start_time": "2025-02-02T10:36:20.208108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model3 = ModelWithSequentialModule(x_train_tensor.shape[1])\n",
    "y_pred_3 = model3(x_train_tensor)\n",
    "print(y_pred_3)"
   ],
   "id": "bf24d3796ecf39ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6207],\n",
      "        [0.6327],\n",
      "        [0.7045],\n",
      "        [0.6807],\n",
      "        [0.6547],\n",
      "        [0.6207],\n",
      "        [0.7351],\n",
      "        [0.6285],\n",
      "        [0.6457],\n",
      "        [0.6937],\n",
      "        [0.6695],\n",
      "        [0.7257],\n",
      "        [0.6497],\n",
      "        [0.6638],\n",
      "        [0.6378],\n",
      "        [0.6316],\n",
      "        [0.6801],\n",
      "        [0.6253],\n",
      "        [0.6445],\n",
      "        [0.6612],\n",
      "        [0.6882],\n",
      "        [0.6681],\n",
      "        [0.6387],\n",
      "        [0.6950],\n",
      "        [0.6731],\n",
      "        [0.6207],\n",
      "        [0.6182],\n",
      "        [0.7103],\n",
      "        [0.6187],\n",
      "        [0.6833],\n",
      "        [0.6207],\n",
      "        [0.6720],\n",
      "        [0.6354],\n",
      "        [0.7641],\n",
      "        [0.6304],\n",
      "        [0.6486],\n",
      "        [0.6566],\n",
      "        [0.6996],\n",
      "        [0.6737],\n",
      "        [0.6312],\n",
      "        [0.6970],\n",
      "        [0.6795],\n",
      "        [0.6873],\n",
      "        [0.7318],\n",
      "        [0.6602],\n",
      "        [0.6469],\n",
      "        [0.7386],\n",
      "        [0.6599],\n",
      "        [0.6477],\n",
      "        [0.7107],\n",
      "        [0.7123],\n",
      "        [0.6342],\n",
      "        [0.6713],\n",
      "        [0.6731],\n",
      "        [0.6478],\n",
      "        [0.6207],\n",
      "        [0.6859],\n",
      "        [0.6405],\n",
      "        [0.6772],\n",
      "        [0.6328],\n",
      "        [0.6526],\n",
      "        [0.6596],\n",
      "        [0.6457],\n",
      "        [0.6698],\n",
      "        [0.6289],\n",
      "        [0.7001],\n",
      "        [0.6464],\n",
      "        [0.6355],\n",
      "        [0.6207],\n",
      "        [0.6436],\n",
      "        [0.7062],\n",
      "        [0.6207],\n",
      "        [0.6591],\n",
      "        [0.6865],\n",
      "        [0.6989],\n",
      "        [0.6667],\n",
      "        [0.6848],\n",
      "        [0.6354],\n",
      "        [0.6715],\n",
      "        [0.7051],\n",
      "        [0.6853],\n",
      "        [0.7302],\n",
      "        [0.6648],\n",
      "        [0.6857],\n",
      "        [0.6768],\n",
      "        [0.6226],\n",
      "        [0.6546],\n",
      "        [0.7134],\n",
      "        [0.6620],\n",
      "        [0.6425],\n",
      "        [0.6448],\n",
      "        [0.6867],\n",
      "        [0.6496],\n",
      "        [0.6421],\n",
      "        [0.6667],\n",
      "        [0.6636],\n",
      "        [0.6735],\n",
      "        [0.7095],\n",
      "        [0.6468],\n",
      "        [0.6841],\n",
      "        [0.6207],\n",
      "        [0.6600],\n",
      "        [0.6295],\n",
      "        [0.7116],\n",
      "        [0.6526],\n",
      "        [0.7084],\n",
      "        [0.6207],\n",
      "        [0.6612],\n",
      "        [0.6207],\n",
      "        [0.6207],\n",
      "        [0.6441],\n",
      "        [0.6693],\n",
      "        [0.6825],\n",
      "        [0.6711],\n",
      "        [0.6771],\n",
      "        [0.6524],\n",
      "        [0.6127],\n",
      "        [0.6610],\n",
      "        [0.7452],\n",
      "        [0.6369],\n",
      "        [0.6525],\n",
      "        [0.6758],\n",
      "        [0.7609],\n",
      "        [0.6308],\n",
      "        [0.6754],\n",
      "        [0.6775],\n",
      "        [0.6335],\n",
      "        [0.6335],\n",
      "        [0.7108],\n",
      "        [0.6290],\n",
      "        [0.6207],\n",
      "        [0.7072],\n",
      "        [0.7185],\n",
      "        [0.6648],\n",
      "        [0.6260],\n",
      "        [0.6431],\n",
      "        [0.6444],\n",
      "        [0.6473],\n",
      "        [0.6121],\n",
      "        [0.6596],\n",
      "        [0.6661],\n",
      "        [0.6207],\n",
      "        [0.6382],\n",
      "        [0.6603],\n",
      "        [0.6723],\n",
      "        [0.6207],\n",
      "        [0.6676],\n",
      "        [0.6207],\n",
      "        [0.6578],\n",
      "        [0.6521],\n",
      "        [0.7258],\n",
      "        [0.6913],\n",
      "        [0.6483],\n",
      "        [0.6885],\n",
      "        [0.6667],\n",
      "        [0.6587],\n",
      "        [0.6364],\n",
      "        [0.6874],\n",
      "        [0.6635],\n",
      "        [0.6597],\n",
      "        [0.6665],\n",
      "        [0.6603],\n",
      "        [0.6569],\n",
      "        [0.6207],\n",
      "        [0.6570],\n",
      "        [0.6314],\n",
      "        [0.6745],\n",
      "        [0.7023],\n",
      "        [0.6648],\n",
      "        [0.6685],\n",
      "        [0.6288],\n",
      "        [0.6524],\n",
      "        [0.6269],\n",
      "        [0.7078],\n",
      "        [0.6655],\n",
      "        [0.6423],\n",
      "        [0.6427],\n",
      "        [0.6262],\n",
      "        [0.6515],\n",
      "        [0.6363],\n",
      "        [0.6660],\n",
      "        [0.7057],\n",
      "        [0.6581],\n",
      "        [0.6257],\n",
      "        [0.6669],\n",
      "        [0.7106],\n",
      "        [0.7072],\n",
      "        [0.6162],\n",
      "        [0.6817],\n",
      "        [0.6414],\n",
      "        [0.6614],\n",
      "        [0.6897],\n",
      "        [0.6917],\n",
      "        [0.6304],\n",
      "        [0.6667],\n",
      "        [0.6888],\n",
      "        [0.6437],\n",
      "        [0.6425],\n",
      "        [0.6793],\n",
      "        [0.6699],\n",
      "        [0.6644],\n",
      "        [0.6498],\n",
      "        [0.6320],\n",
      "        [0.6339],\n",
      "        [0.6852],\n",
      "        [0.6808],\n",
      "        [0.7683],\n",
      "        [0.6824],\n",
      "        [0.7349],\n",
      "        [0.6288],\n",
      "        [0.6396],\n",
      "        [0.6568],\n",
      "        [0.6892],\n",
      "        [0.6423],\n",
      "        [0.6152],\n",
      "        [0.6848],\n",
      "        [0.6532],\n",
      "        [0.7544],\n",
      "        [0.7082],\n",
      "        [0.6838],\n",
      "        [0.7232],\n",
      "        [0.6558],\n",
      "        [0.6555],\n",
      "        [0.6731],\n",
      "        [0.7106],\n",
      "        [0.6590],\n",
      "        [0.7259],\n",
      "        [0.6448],\n",
      "        [0.6314],\n",
      "        [0.6154],\n",
      "        [0.6842],\n",
      "        [0.6247],\n",
      "        [0.6381],\n",
      "        [0.6612],\n",
      "        [0.6570],\n",
      "        [0.6269],\n",
      "        [0.6384],\n",
      "        [0.6911],\n",
      "        [0.6152],\n",
      "        [0.6369],\n",
      "        [0.6489],\n",
      "        [0.6433],\n",
      "        [0.6663],\n",
      "        [0.6329],\n",
      "        [0.6971],\n",
      "        [0.7094],\n",
      "        [0.7521],\n",
      "        [0.6408],\n",
      "        [0.6516],\n",
      "        [0.6362],\n",
      "        [0.7028],\n",
      "        [0.6583],\n",
      "        [0.7501],\n",
      "        [0.6529],\n",
      "        [0.6784],\n",
      "        [0.6723],\n",
      "        [0.6054],\n",
      "        [0.6297],\n",
      "        [0.7523],\n",
      "        [0.7044],\n",
      "        [0.6210],\n",
      "        [0.6981],\n",
      "        [0.6653],\n",
      "        [0.6207],\n",
      "        [0.7216],\n",
      "        [0.6400],\n",
      "        [0.6454],\n",
      "        [0.6709],\n",
      "        [0.6207],\n",
      "        [0.6645],\n",
      "        [0.6711],\n",
      "        [0.6722],\n",
      "        [0.6987],\n",
      "        [0.6875],\n",
      "        [0.6885],\n",
      "        [0.6569],\n",
      "        [0.6333],\n",
      "        [0.6207],\n",
      "        [0.6377],\n",
      "        [0.7362],\n",
      "        [0.6426],\n",
      "        [0.6885],\n",
      "        [0.6355],\n",
      "        [0.6207],\n",
      "        [0.6330],\n",
      "        [0.6207],\n",
      "        [0.6616],\n",
      "        [0.6757],\n",
      "        [0.6224],\n",
      "        [0.6731],\n",
      "        [0.6665],\n",
      "        [0.6445],\n",
      "        [0.6444],\n",
      "        [0.6488],\n",
      "        [0.6207],\n",
      "        [0.6618],\n",
      "        [0.6347],\n",
      "        [0.6393],\n",
      "        [0.6656],\n",
      "        [0.6497],\n",
      "        [0.6452],\n",
      "        [0.6621],\n",
      "        [0.6560],\n",
      "        [0.6617],\n",
      "        [0.6762],\n",
      "        [0.6620],\n",
      "        [0.6568],\n",
      "        [0.6762],\n",
      "        [0.6635],\n",
      "        [0.6782],\n",
      "        [0.6294],\n",
      "        [0.6139],\n",
      "        [0.6353],\n",
      "        [0.6488],\n",
      "        [0.6461],\n",
      "        [0.7033],\n",
      "        [0.6839],\n",
      "        [0.6787],\n",
      "        [0.7108],\n",
      "        [0.6337],\n",
      "        [0.7810],\n",
      "        [0.6356],\n",
      "        [0.6515],\n",
      "        [0.6554],\n",
      "        [0.6628],\n",
      "        [0.6083],\n",
      "        [0.6207],\n",
      "        [0.8102],\n",
      "        [0.6411],\n",
      "        [0.6396],\n",
      "        [0.6399],\n",
      "        [0.6775],\n",
      "        [0.6207],\n",
      "        [0.6901],\n",
      "        [0.6599],\n",
      "        [0.6365],\n",
      "        [0.6207],\n",
      "        [0.6538],\n",
      "        [0.6758],\n",
      "        [0.6316],\n",
      "        [0.6415],\n",
      "        [0.6485],\n",
      "        [0.7235],\n",
      "        [0.6131],\n",
      "        [0.6908],\n",
      "        [0.6476],\n",
      "        [0.6662],\n",
      "        [0.7139],\n",
      "        [0.6717],\n",
      "        [0.6878],\n",
      "        [0.6533],\n",
      "        [0.6735],\n",
      "        [0.7038],\n",
      "        [0.6207],\n",
      "        [0.6545],\n",
      "        [0.6997],\n",
      "        [0.7030],\n",
      "        [0.6207],\n",
      "        [0.6570],\n",
      "        [0.6313],\n",
      "        [0.6207],\n",
      "        [0.6996],\n",
      "        [0.6207],\n",
      "        [0.6762],\n",
      "        [0.6543],\n",
      "        [0.6207],\n",
      "        [0.6703],\n",
      "        [0.6989],\n",
      "        [0.6724],\n",
      "        [0.6662],\n",
      "        [0.6267],\n",
      "        [0.6513],\n",
      "        [0.6381],\n",
      "        [0.6594],\n",
      "        [0.6801],\n",
      "        [0.6763],\n",
      "        [0.8569],\n",
      "        [0.6489],\n",
      "        [0.6569],\n",
      "        [0.6532],\n",
      "        [0.6738],\n",
      "        [0.7633],\n",
      "        [0.6826],\n",
      "        [0.6811],\n",
      "        [0.6377],\n",
      "        [0.6630],\n",
      "        [0.6488],\n",
      "        [0.6148],\n",
      "        [0.6717],\n",
      "        [0.7160],\n",
      "        [0.7979],\n",
      "        [0.6207],\n",
      "        [0.6747],\n",
      "        [0.6323],\n",
      "        [0.6364],\n",
      "        [0.6666],\n",
      "        [0.6424],\n",
      "        [0.6350],\n",
      "        [0.6714],\n",
      "        [0.7381],\n",
      "        [0.7419],\n",
      "        [0.6381],\n",
      "        [0.6553],\n",
      "        [0.6833],\n",
      "        [0.6454],\n",
      "        [0.7076],\n",
      "        [0.6806],\n",
      "        [0.6345],\n",
      "        [0.7019],\n",
      "        [0.7202],\n",
      "        [0.7313],\n",
      "        [0.6941],\n",
      "        [0.6274],\n",
      "        [0.7304],\n",
      "        [0.6448],\n",
      "        [0.6544],\n",
      "        [0.6478],\n",
      "        [0.6435],\n",
      "        [0.7106],\n",
      "        [0.7078],\n",
      "        [0.7039],\n",
      "        [0.6207],\n",
      "        [0.6799],\n",
      "        [0.6599],\n",
      "        [0.6491],\n",
      "        [0.7027],\n",
      "        [0.6524],\n",
      "        [0.6207],\n",
      "        [0.7452],\n",
      "        [0.6421],\n",
      "        [0.6240],\n",
      "        [0.7104],\n",
      "        [0.6765],\n",
      "        [0.6384],\n",
      "        [0.6802],\n",
      "        [0.6971],\n",
      "        [0.6592],\n",
      "        [0.6920],\n",
      "        [0.6879],\n",
      "        [0.6483],\n",
      "        [0.6449],\n",
      "        [0.6511],\n",
      "        [0.7447],\n",
      "        [0.6657],\n",
      "        [0.6658],\n",
      "        [0.7021],\n",
      "        [0.6910],\n",
      "        [0.6751],\n",
      "        [0.7079],\n",
      "        [0.6179],\n",
      "        [0.6885],\n",
      "        [0.6305],\n",
      "        [0.6561],\n",
      "        [0.6430],\n",
      "        [0.6571]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:20.283451Z",
     "start_time": "2025-02-02T10:36:20.280094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.network(features)\n",
    "\n",
    "\n",
    "    def backward(self, features, target, learning_rate, epochs):\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.BCELoss()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = self.forward(features)\n",
    "\n",
    "            loss = loss_fn(y_pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'epoch: {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "    def predict(self, features):\n",
    "        with torch.no_grad():\n",
    "            y_prediction = self.forward(features)\n",
    "            return (y_prediction > 0.5).float()\n",
    "\n",
    "\n",
    "    def accuracy(self, features, target):\n",
    "        y_pred_class = self.predict(features)\n",
    "        return  (y_pred_class == target).float().mean().item()\n",
    "\n"
   ],
   "id": "1c350dc7b8989fd",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:26.603716Z",
     "start_time": "2025-02-02T10:36:26.593562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = NNClassifier(x_train_tensor.shape[1])\n",
    "\n",
    "classifier.backward(x_train_tensor, y_train_tensor.view(-1, 1), learning_rate=0.1, epochs=25)"
   ],
   "id": "27a56c89f50990e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Loss: 0.6235623359680176\n",
      "epoch: 2, Loss: 0.6235623359680176\n",
      "epoch: 3, Loss: 0.6235623359680176\n",
      "epoch: 4, Loss: 0.6235623359680176\n",
      "epoch: 5, Loss: 0.6235623359680176\n",
      "epoch: 6, Loss: 0.6235623359680176\n",
      "epoch: 7, Loss: 0.6235623359680176\n",
      "epoch: 8, Loss: 0.6235623359680176\n",
      "epoch: 9, Loss: 0.6235623359680176\n",
      "epoch: 10, Loss: 0.6235623359680176\n",
      "epoch: 11, Loss: 0.6235623359680176\n",
      "epoch: 12, Loss: 0.6235623359680176\n",
      "epoch: 13, Loss: 0.6235623359680176\n",
      "epoch: 14, Loss: 0.6235623359680176\n",
      "epoch: 15, Loss: 0.6235623359680176\n",
      "epoch: 16, Loss: 0.6235623359680176\n",
      "epoch: 17, Loss: 0.6235623359680176\n",
      "epoch: 18, Loss: 0.6235623359680176\n",
      "epoch: 19, Loss: 0.6235623359680176\n",
      "epoch: 20, Loss: 0.6235623359680176\n",
      "epoch: 21, Loss: 0.6235623359680176\n",
      "epoch: 22, Loss: 0.6235623359680176\n",
      "epoch: 23, Loss: 0.6235623359680176\n",
      "epoch: 24, Loss: 0.6235623359680176\n",
      "epoch: 25, Loss: 0.6235623359680176\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- When you use an optimizer (e.g., SGD, Adam), you don't need to manually zero out gradients for each layer in your model. The optimizer will automatically handle the gradient zeroing for all parameters it is responsible for.\n",
    "- optimizer.zero_grad() will automatically zero out the gradients of all parameters in your model before each backward pass. This is part of the standard practice in PyTorch when using optimizers.\n",
    "- The reason we can rely on optimizer.zero_grad() instead of manually calling .zero_() on the individual layers is that optimizer.zero_grad() works with the parameters registered with the optimizer, which typically includes all model parameters (e.g., weights and biases)."
   ],
   "id": "3dbaabc97579695d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:28.285162Z",
     "start_time": "2025-02-02T10:36:28.282325Z"
    }
   },
   "cell_type": "code",
   "source": "classifier.eval()",
   "id": "a85689440a8c3d8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNClassifier(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T10:36:28.855846Z",
     "start_time": "2025-02-02T10:36:28.851966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = classifier.accuracy(x_test_tensor, y_test_tensor)\n",
    "print(accuracy)"
   ],
   "id": "a4cc6552948f4906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6206524968147278\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
