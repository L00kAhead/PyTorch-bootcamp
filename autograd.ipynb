{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1 align=\"center\">PyTorch Autograd</h1>\n",
    "\n",
    "- Autograd is a core component of PyTorch that provide automatic differentiation for tensor operations.\n",
    "- it enables gradiant computation, which is essential for training models using optimization algorithms like Gradiant Descent."
   ],
   "id": "c8ff1871cfafde72"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T18:22:53.101466Z",
     "start_time": "2025-01-30T18:22:52.161485Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:22:54.034246Z",
     "start_time": "2025-01-30T18:22:54.027954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor(10, requires_grad=True, dtype=torch.float16)\n",
    "y = x ** 2\n",
    "\n",
    "print(x)"
   ],
   "id": "47ebb9b9d5e70c21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:22:55.316983Z",
     "start_time": "2025-01-30T18:22:55.313047Z"
    }
   },
   "cell_type": "code",
   "source": "print(y)",
   "id": "ca1ad7139faa270d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100., dtype=torch.float16, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:22:57.460155Z",
     "start_time": "2025-01-30T18:22:57.435041Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "faf4dbd4b7b3f72a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:22:58.748083Z",
     "start_time": "2025-01-30T18:22:58.741461Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "d1c7a95ceca8c2a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Forward Propagation : X -- (x) **2 --> y\n",
    "    - 10 -- (10) ** 2 --> 100\n",
    "- Backward Propagation : dy/dx = d(x^2)/dx = 2 * x\n",
    "    - 10 -- 2(10) --> 20"
   ],
   "id": "4f4a31aee775a4ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:56:47.184987Z",
     "start_time": "2025-01-30T18:56:47.178405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple NN with no hidden layer\n",
    "# Sigmoid activation function and Binary Cross Entropy Loss Fn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "w, b"
   ],
   "id": "f8aa7fe5c33d8995",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., requires_grad=True), tensor(0., requires_grad=True))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:56:47.445487Z",
     "start_time": "2025-01-30T18:56:47.442299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binary_cross_entropy(prediction, target):\n",
    "    epsilon = 1e-8\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
   ],
   "id": "13c890b24e8c150a",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T18:56:47.848237Z",
     "start_time": "2025-01-30T18:56:47.844907Z"
    }
   },
   "cell_type": "code",
   "source": "y = torch.tensor(0.0)",
   "id": "85802cf19f9dd2f5",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:04:43.429534Z",
     "start_time": "2025-01-30T19:04:43.424974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.tensor(6.7)\n",
    "print(x)\n",
    "\n",
    "# FORWARD PROPAGATION\n",
    "# dot product of weight and input and add bias\n",
    "z = w * X + b\n",
    "print(\"Z: \", z)\n",
    "\n",
    "# apply sigmoid activation function since it's classification\n",
    "y_pred = torch.sigmoid(z)\n",
    "print(\"Y_Pred: \", y_pred)\n",
    "\n",
    "# using BCE since it is binary classification\n",
    "loss = binary_cross_entropy(y_pred, y)\n",
    "print(\"Loss: \", loss)\n",
    "\n",
    "# BACKWARD PROPAGATION\n",
    "# compute gradients\n",
    "loss.backward()"
   ],
   "id": "590f658b4bf40464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6029])\n",
      "Z:  tensor(6.7000, grad_fn=<AddBackward0>)\n",
      "Y_Pred:  tensor(0.9988, grad_fn=<SigmoidBackward0>)\n",
      "Loss:  tensor(6.7012, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "w -- * -- x --- +(b) --> z ---> sigmoid(z) --> y_pred --- y --> Loss Function ---> Loss",
   "id": "55c542923cdd1a01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you rerun the upper cell and run the next cell you will notice that the value of gradients w and  b will change from 6.6918 and 0.9988 (after each run). To overcome this issue we can manually set the grads of w and b to 0. `w.grad.zero()` or use `with torch.no_grad()`",
   "id": "1f42bee258b76d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:04:56.307369Z",
     "start_time": "2025-01-30T19:04:56.302535Z"
    }
   },
   "cell_type": "code",
   "source": "w.grad, b.grad",
   "id": "28dc5ec8753a9609",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.6918), tensor(0.9988))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:04:57.293672Z",
     "start_time": "2025-01-30T19:04:57.288863Z"
    }
   },
   "cell_type": "code",
   "source": "w.grad.zero_(), b.grad.zero_()",
   "id": "80419266195b5c66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "714d679618670204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After training the model, during prediction we don't need back propagation in that case we can turn off the gradiant calculation.",
   "id": "e1bd0b3a799a1f87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:07:57.909275Z",
     "start_time": "2025-01-30T19:07:57.903310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()"
   ],
   "id": "c42acd3aa593436",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# option 1: set require_grad = False\n",
    "x.requires_grad_(False)"
   ],
   "id": "78d3bbb9ee8a4b4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:10:50.659817Z",
     "start_time": "2025-01-30T19:10:50.498396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# option 2: detach()\n",
    "x_new = x.detach()\n",
    "print(x_new)\n",
    "\n",
    "y = x_new ** 2\n",
    "y.backward()"
   ],
   "id": "4c197f017a7fa57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[125], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_new)\n\u001B[1;32m      5\u001B[0m y \u001B[38;5;241m=\u001B[39m x_new \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 6\u001B[0m y\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    583\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:11:48.844882Z",
     "start_time": "2025-01-30T19:11:48.820362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# option 3: torch_no_grad()\n",
    "\n",
    "x = torch.tensor(6.7, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "\n",
    "y.backward() # Error"
   ],
   "id": "11786e4c937fa901",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[126], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      6\u001B[0m     y \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 8\u001B[0m y\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    583\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3bc71d7a59af70d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
